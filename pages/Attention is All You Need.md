- ![Transformer Attention.webp](../assets/Transformer_Attention_1767107223688_0.webp){:height 541, :width 778}
- The core of the transformer is the encorder and decorder. Encorder is a two-layer structure with multi-head attention layer as the first, and position-wise feed forward network, while decorder has the same structure in the front, but adding two extra layer linear and softmax. The attention in the encorder is embedded with positional information and add N sets of parallel multi-head attention and FFN. In decorder two attention mechanisms are added. One is similar as the encorder to embedded the output word, and send it to the high dimensional space. It benefits the next attention -- cross-attention, to find the relationship between encorder's self-attention and decorder's self-attention