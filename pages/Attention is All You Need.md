- ![Transformer Attention.webp](../assets/Transformer_Attention_1767107223688_0.webp){:height 541, :width 778}
- The core of the transformer is the encorder and decorder. Encorder is a two-layer structure with multi-head attention layer as the first, and position-wise feed forward network, while decorder has the same structure in the front, but adding two extra layer linear and softmax. The attention in the encorder is embedded with positional information and add N sets of parallel multi-head attention and FFN.