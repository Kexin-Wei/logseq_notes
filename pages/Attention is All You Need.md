- ![Transformer Attention.webp](../assets/Transformer_Attention_1767107223688_0.webp){:height 541, :width 778}
- The core of the Transformer consists of the encorder and decorder. The cncorder is a two-layer structure comprising a multi-head attention layer followed by a position-wise feed forward network. This structure is repeated N times with residual connections. The decorder shares a similar architecture but includes two extra layer linear and softmax. The attention in the encorder is embedded with positional information and add N sets of parallel multi-head attention and FFN. In decorder two attention mechanisms are added. One is similar as the encorder to embedded the output word, and send it to the high dimensional space. It benefits the next attention -- cross-attention, to find the relationship between encorder's self-attention and decorder's self-attention. The last two layer of the decorder is to map the high dimensional information back to vocabulary and send out a distribution to match the output's correctness and adjust the weights.